# CRITICAL Backpressure Alert: Complete PromQL and Grafana Implementation

**Document Type:** Internal Engineering Specification  
**Author:** Platform Technical Lead  
**Status:** Production-Ready  
**Last Updated:** 2026-01-15

---

## 1. Alert Overview

The CRITICAL backpressure alert triggers when system utilization exceeds 95%, indicating imminent capacity exhaustion. At this level, the system enters emergency mode: rejecting new connections, dropping low-priority operations, and alerting on-call engineers.

**Alert Characteristics:**
- **Threshold:** 95% utilization
- **Evaluation Interval:** 15 seconds
- **For Duration:** 1 minute (4 consecutive breaches)
- **Severity:** critical (pages on-call)
- **Auto-Resolution:** Yes, when utilization drops below 90% for 2 minutes

---

## 2. Prometheus Alert Rules

### 2.1 Primary CRITICAL Alert Rule

```yaml
# File: /etc/prometheus/rules/collaboration_backpressure.yml

groups:
  - name: collaboration_backpressure_critical
    interval: 15s
    rules:
      # ============================================================
      # CRITICAL: Overall System Backpressure (95%+)
      # ============================================================
      - alert: CollaborationBackpressureCritical
        expr: |
          (
            # Weighted average of all resource utilizations
            (
              # WebSocket connection utilization (weight: 0.3)
              0.3 * (
                sum(collab_websocket_active_connections)
                /
                sum(collab_websocket_max_connections)
              )
              +
              # Redis pub/sub channel utilization (weight: 0.25)
              0.25 * (
                sum(collab_redis_pubsub_channels_active)
                /
                sum(collab_redis_pubsub_channels_max)
              )
              +
              # OT operation queue utilization (weight: 0.25)
              0.25 * (
                sum(collab_ot_queue_depth)
                /
                sum(collab_ot_queue_capacity)
              )
              +
              # Memory utilization (weight: 0.2)
              0.2 * (
                sum(container_memory_working_set_bytes{namespace="collaboration"})
                /
                sum(container_spec_memory_limit_bytes{namespace="collaboration"})
              )
            )
          ) > 0.95
        for: 1m
        labels:
          severity: critical
          team: platform
          component: collaboration
          escalation_policy: immediate
        annotations:
          summary: "CRITICAL: Collaboration system at {{ $value | humanizePercentage }} capacity"
          description: |
            Collaboration system has exceeded 95% capacity for over 1 minute.
            
            IMMEDIATE ACTIONS REQUIRED:
            1. Check WebSocket connections: {{ with query "sum(collab_websocket_active_connections)" }}{{ . | first | value }}{{ end }}
            2. Check OT queue depth: {{ with query "sum(collab_ot_queue_depth)" }}{{ . | first | value }}{{ end }}
            3. Check Redis channels: {{ with query "sum(collab_redis_pubsub_channels_active)" }}{{ . | first | value }}{{ end }}
            
            System is in EMERGENCY MODE - rejecting new connections.
          runbook_url: "https://runbooks.internal/collaboration/critical-backpressure"
          dashboard_url: "https://grafana.internal/d/collab-backpressure"

      # ============================================================
      # CRITICAL: WebSocket Connection Exhaustion
      # ============================================================
      - alert: WebSocketConnectionsCritical
        expr: |
          (
            sum by (pod) (collab_websocket_active_connections)
            /
            sum by (pod) (collab_websocket_max_connections)
          ) > 0.95
        for: 30s
        labels:
          severity: critical
          team: platform
          component: websocket
        annotations:
          summary: "CRITICAL: WebSocket connections at {{ $value | humanizePercentage }} on {{ $labels.pod }}"
          description: |
            Pod {{ $labels.pod }} has exhausted WebSocket connections.
            Active: {{ with query "collab_websocket_active_connections{pod='{{ $labels.pod }}'}" }}{{ . | first | value }}{{ end }}
            Max: {{ with query "collab_websocket_max_connections{pod='{{ $labels.pod }}'}" }}{{ . | first | value }}{{ end }}
            
            New connections are being REJECTED.
          runbook_url: "https://runbooks.internal/collaboration/websocket-exhaustion"

      # ============================================================
      # CRITICAL: OT Queue Overflow
      # ============================================================
      - alert: OTQueueOverflowCritical
        expr: |
          (
            sum by (workspace_id) (collab_ot_queue_depth)
            /
            sum by (workspace_id) (collab_ot_queue_capacity)
          ) > 0.95
        for: 30s
        labels:
          severity: critical
          team: platform
          component: ot_engine
        annotations:
          summary: "CRITICAL: OT queue overflow for workspace {{ $labels.workspace_id }}"
          description: |
            Workspace {{ $labels.workspace_id }} OT queue is at {{ $value | humanizePercentage }}.
            Operations are being DROPPED.
            
            Possible causes:
            - Edit flood attack
            - Slow consumer
            - Network partition
          runbook_url: "https://runbooks.internal/collaboration/ot-queue-overflow"

      # ============================================================
      # CRITICAL: Redis Pub/Sub Saturation
      # ============================================================
      - alert: RedisPubSubCritical
        expr: |
          (
            sum by (redis_node) (collab_redis_pubsub_channels_active)
            /
            sum by (redis_node) (collab_redis_pubsub_channels_max)
          ) > 0.95
        for: 30s
        labels:
          severity: critical
          team: platform
          component: redis
        annotations:
          summary: "CRITICAL: Redis pub/sub at {{ $value | humanizePercentage }} on {{ $labels.redis_node }}"
          description: |
            Redis node {{ $labels.redis_node }} pub/sub channels exhausted.
            New workspace subscriptions will FAIL.
          runbook_url: "https://runbooks.internal/collaboration/redis-pubsub-saturation"

      # ============================================================
      # CRITICAL: Memory Pressure
      # ============================================================
      - alert: CollaborationMemoryCritical
        expr: |
          (
            sum by (pod) (container_memory_working_set_bytes{namespace="collaboration"})
            /
            sum by (pod) (container_spec_memory_limit_bytes{namespace="collaboration"})
          ) > 0.95
        for: 1m
        labels:
          severity: critical
          team: platform
          component: memory
        annotations:
          summary: "CRITICAL: Memory at {{ $value | humanizePercentage }} on {{ $labels.pod }}"
          description: |
            Pod {{ $labels.pod }} is about to OOM.
            Immediate action required to prevent crash.
          runbook_url: "https://runbooks.internal/collaboration/memory-critical"
```

### 2.2 Backpressure Level Recording Rules

```yaml
# File: /etc/prometheus/rules/collaboration_recording.yml

groups:
  - name: collaboration_backpressure_recording
    interval: 15s
    rules:
      # ============================================================
      # Recording Rules for Dashboard Efficiency
      # ============================================================
      
      # Overall backpressure level (0-1)
      - record: collab:backpressure_level:ratio
        expr: |
          (
            0.3 * (sum(collab_websocket_active_connections) / sum(collab_websocket_max_connections))
            +
            0.25 * (sum(collab_redis_pubsub_channels_active) / sum(collab_redis_pubsub_channels_max))
            +
            0.25 * (sum(collab_ot_queue_depth) / sum(collab_ot_queue_capacity))
            +
            0.2 * (sum(container_memory_working_set_bytes{namespace="collaboration"}) / sum(container_spec_memory_limit_bytes{namespace="collaboration"}))
          )

      # Backpressure level category (0=NORMAL, 1=ELEVATED, 2=HIGH, 3=CRITICAL)
      - record: collab:backpressure_category:level
        expr: |
          (
            (collab:backpressure_level:ratio >= 0.95) * 3
            +
            (collab:backpressure_level:ratio >= 0.85 and collab:backpressure_level:ratio < 0.95) * 2
            +
            (collab:backpressure_level:ratio >= 0.70 and collab:backpressure_level:ratio < 0.85) * 1
            +
            (collab:backpressure_level:ratio < 0.70) * 0
          )

      # Per-resource utilization
      - record: collab:websocket_utilization:ratio
        expr: |
          sum(collab_websocket_active_connections) / sum(collab_websocket_max_connections)

      - record: collab:redis_pubsub_utilization:ratio
        expr: |
          sum(collab_redis_pubsub_channels_active) / sum(collab_redis_pubsub_channels_max)

      - record: collab:ot_queue_utilization:ratio
        expr: |
          sum(collab_ot_queue_depth) / sum(collab_ot_queue_capacity)

      - record: collab:memory_utilization:ratio
        expr: |
          sum(container_memory_working_set_bytes{namespace="collaboration"}) 
          / 
          sum(container_spec_memory_limit_bytes{namespace="collaboration"})

      # Operations rejected due to backpressure (rate)
      - record: collab:ops_rejected:rate5m
        expr: |
          sum(rate(collab_operations_rejected_total[5m]))

      # Connections rejected due to backpressure (rate)
      - record: collab:connections_rejected:rate5m
        expr: |
          sum(rate(collab_websocket_connections_rejected_total[5m]))
```

---

## 3. PromQL Queries Explained

### 3.1 Primary Backpressure Query

```promql
# Query: Overall system backpressure level
# Returns: 0.0 to 1.0 (percentage as decimal)

(
  # WebSocket connection utilization (weight: 30%)
  # Why 30%: WebSocket exhaustion immediately blocks new users
  0.3 * (
    sum(collab_websocket_active_connections)
    /
    sum(collab_websocket_max_connections)
  )
  +
  # Redis pub/sub channel utilization (weight: 25%)
  # Why 25%: Channel exhaustion blocks workspace subscriptions
  0.25 * (
    sum(collab_redis_pubsub_channels_active)
    /
    sum(collab_redis_pubsub_channels_max)
  )
  +
  # OT operation queue utilization (weight: 25%)
  # Why 25%: Queue overflow causes operation drops
  0.25 * (
    sum(collab_ot_queue_depth)
    /
    sum(collab_ot_queue_capacity)
  )
  +
  # Memory utilization (weight: 20%)
  # Why 20%: Memory pressure causes OOM but has some buffer
  0.2 * (
    sum(container_memory_working_set_bytes{namespace="collaboration"})
    /
    sum(container_spec_memory_limit_bytes{namespace="collaboration"})
  )
)
```

### 3.2 Component Breakdown Queries

```promql
# WebSocket utilization by pod
sum by (pod) (collab_websocket_active_connections)
/
sum by (pod) (collab_websocket_max_connections)

# Redis utilization by node
sum by (redis_node) (collab_redis_pubsub_channels_active)
/
sum by (redis_node) (collab_redis_pubsub_channels_max)

# OT queue utilization by workspace (top 10)
topk(10,
  sum by (workspace_id) (collab_ot_queue_depth)
  /
  sum by (workspace_id) (collab_ot_queue_capacity)
)

# Memory utilization by pod
sum by (pod) (container_memory_working_set_bytes{namespace="collaboration"})
/
sum by (pod) (container_spec_memory_limit_bytes{namespace="collaboration"})
```

### 3.3 Trend and Prediction Queries

```promql
# Backpressure trend (is it increasing?)
deriv(collab:backpressure_level:ratio[10m])

# Predict when CRITICAL will be reached (linear extrapolation)
# Returns seconds until 95% if current trend continues
(0.95 - collab:backpressure_level:ratio)
/
deriv(collab:backpressure_level:ratio[10m])

# Rate of rejected operations
sum(rate(collab_operations_rejected_total{reason="backpressure"}[5m]))

# Rate of rejected connections
sum(rate(collab_websocket_connections_rejected_total{reason="backpressure"}[5m]))
```

### 3.4 Historical Comparison Queries

```promql
# Current vs 24h ago
collab:backpressure_level:ratio
-
collab:backpressure_level:ratio offset 24h

# Current vs 7d ago (same time of day)
collab:backpressure_level:ratio
-
collab:backpressure_level:ratio offset 7d

# Max backpressure in last 24h
max_over_time(collab:backpressure_level:ratio[24h])

# Time spent in CRITICAL in last 24h (seconds)
sum_over_time((collab:backpressure_level:ratio > 0.95)[24h:1m]) * 60
```

---

## 4. Grafana Dashboard Definition

### 4.1 Complete Dashboard JSON

```json
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": "-- Grafana --",
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      },
      {
        "datasource": "Prometheus",
        "enable": true,
        "expr": "ALERTS{alertname=~\".*Backpressure.*|.*Critical.*\", alertstate=\"firing\"}",
        "iconColor": "#E02F44",
        "name": "Backpressure Alerts",
        "tagKeys": "alertname,severity",
        "titleFormat": "{{ alertname }}"
      }
    ]
  },
  "description": "Real-time collaboration system backpressure monitoring",
  "editable": true,
  "gnetId": null,
  "graphTooltip": 1,
  "id": null,
  "iteration": 1705312800000,
  "links": [
    {
      "icon": "doc",
      "tags": [],
      "targetBlank": true,
      "title": "Runbook",
      "tooltip": "Backpressure Runbook",
      "type": "link",
      "url": "https://runbooks.internal/collaboration/backpressure"
    }
  ],
  "panels": [
    {
      "id": 1,
      "title": "üö® BACKPRESSURE STATUS",
      "type": "stat",
      "gridPos": { "h": 4, "w": 6, "x": 0, "y": 0 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "collab:backpressure_level:ratio * 100",
          "legendFormat": "Backpressure %",
          "refId": "A"
        }
      ],
      "options": {
        "reduceOptions": {
          "values": false,
          "calcs": ["lastNotNull"],
          "fields": ""
        },
        "orientation": "auto",
        "textMode": "auto",
        "colorMode": "background",
        "graphMode": "area",
        "justifyMode": "auto"
      },
      "fieldConfig": {
        "defaults": {
          "unit": "percent",
          "decimals": 1,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              { "color": "#73BF69", "value": null },
              { "color": "#FADE2A", "value": 70 },
              { "color": "#FF9830", "value": 85 },
              { "color": "#E02F44", "value": 95 }
            ]
          },
          "mappings": [
            {
              "type": "range",
              "options": {
                "from": 0,
                "to": 70,
                "result": { "text": "NORMAL", "color": "#73BF69" }
              }
            },
            {
              "type": "range",
              "options": {
                "from": 70,
                "to": 85,
                "result": { "text": "ELEVATED", "color": "#FADE2A" }
              }
            },
            {
              "type": "range",
              "options": {
                "from": 85,
                "to": 95,
                "result": { "text": "HIGH", "color": "#FF9830" }
              }
            },
            {
              "type": "range",
              "options": {
                "from": 95,
                "to": 100,
                "result": { "text": "üî• CRITICAL", "color": "#E02F44" }
              }
            }
          ]
        },
        "overrides": []
      }
    },
    {
      "id": 2,
      "title": "Backpressure Category",
      "type": "stat",
      "gridPos": { "h": 4, "w": 6, "x": 6, "y": 0 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "collab:backpressure_category:level",
          "legendFormat": "Level",
          "refId": "A"
        }
      ],
      "options": {
        "reduceOptions": {
          "values": false,
          "calcs": ["lastNotNull"],
          "fields": ""
        },
        "colorMode": "background",
        "graphMode": "none"
      },
      "fieldConfig": {
        "defaults": {
          "mappings": [
            { "type": "value", "options": { "0": { "text": "‚úÖ NORMAL", "color": "#73BF69" } } },
            { "type": "value", "options": { "1": { "text": "‚ö†Ô∏è ELEVATED", "color": "#FADE2A" } } },
            { "type": "value", "options": { "2": { "text": "üî∂ HIGH", "color": "#FF9830" } } },
            { "type": "value", "options": { "3": { "text": "üî• CRITICAL", "color": "#E02F44" } } }
          ],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              { "color": "#73BF69", "value": null },
              { "color": "#FADE2A", "value": 1 },
              { "color": "#FF9830", "value": 2 },
              { "color": "#E02F44", "value": 3 }
            ]
          }
        }
      }
    },
    {
      "id": 3,
      "title": "Time to CRITICAL (Prediction)",
      "type": "stat",
      "gridPos": { "h": 4, "w": 6, "x": 12, "y": 0 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "clamp_min((0.95 - collab:backpressure_level:ratio) / deriv(collab:backpressure_level:ratio[10m]), 0)",
          "legendFormat": "Time to CRITICAL",
          "refId": "A"
        }
      ],
      "options": {
        "reduceOptions": {
          "values": false,
          "calcs": ["lastNotNull"],
          "fields": ""
        },
        "colorMode": "background",
        "graphMode": "none"
      },
      "fieldConfig": {
        "defaults": {
          "unit": "s",
          "decimals": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              { "color": "#E02F44", "value": null },
              { "color": "#FF9830", "value": 300 },
              { "color": "#FADE2A", "value": 900 },
              { "color": "#73BF69", "value": 1800 }
            ]
          },
          "mappings": [
            {
              "type": "special",
              "options": {
                "match": "null",
                "result": { "text": "‚àû (stable)", "color": "#73BF69" }
              }
            },
            {
              "type": "special",
              "options": {
                "match": "NaN",
                "result": { "text": "‚àû (decreasing)", "color": "#73BF69" }
              }
            }
          ]
        }
      }
    },
    {
      "id": 4,
      "title": "Ops Rejected/sec",
      "type": "stat",
      "gridPos": { "h": 4, "w": 6, "x": 18, "y": 0 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "sum(rate(collab_operations_rejected_total{reason=\"backpressure\"}[1m]))",
          "legendFormat": "Rejected/sec",
          "refId": "A"
        }
      ],
      "options": {
        "reduceOptions": {
          "values": false,
          "calcs": ["lastNotNull"],
          "fields": ""
        },
        "colorMode": "background",
        "graphMode": "area"
      },
      "fieldConfig": {
        "defaults": {
          "unit": "ops",
          "decimals": 1,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              { "color": "#73BF69", "value": null },
              { "color": "#FADE2A", "value": 1 },
              { "color": "#FF9830", "value": 10 },
              { "color": "#E02F44", "value": 100 }
            ]
          }
        }
      }
    },
    {
      "id": 5,
      "title": "Backpressure Level Over Time",
      "type": "timeseries",
      "gridPos": { "h": 8, "w": 24, "x": 0, "y": 4 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "collab:backpressure_level:ratio * 100",
          "legendFormat": "Overall Backpressure",
          "refId": "A"
        },
        {
          "expr": "collab:websocket_utilization:ratio * 100",
          "legendFormat": "WebSocket",
          "refId": "B"
        },
        {
          "expr": "collab:redis_pubsub_utilization:ratio * 100",
          "legendFormat": "Redis Pub/Sub",
          "refId": "C"
        },
        {
          "expr": "collab:ot_queue_utilization:ratio * 100",
          "legendFormat": "OT Queue",
          "refId": "D"
        },
        {
          "expr": "collab:memory_utilization:ratio * 100",
          "legendFormat": "Memory",
          "refId": "E"
        }
      ],
      "options": {
        "tooltip": { "mode": "multi", "sort": "desc" },
        "legend": { "displayMode": "table", "placement": "right", "calcs": ["lastNotNull", "max"] }
      },
      "fieldConfig": {
        "defaults": {
          "unit": "percent",
          "min": 0,
          "max": 100,
          "custom": {
            "drawStyle": "line",
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "fillOpacity": 10,
            "gradientMode": "opacity",
            "spanNulls": true,
            "showPoints": "never",
            "thresholdsStyle": { "mode": "dashed+area" }
          },
          "thresholds": {
            "mode": "absolute",
            "steps": [
              { "color": "transparent", "value": null },
              { "color": "rgba(250, 222, 42, 0.1)", "value": 70 },
              { "color": "rgba(255, 152, 48, 0.2)", "value": 85 },
              { "color": "rgba(224, 47, 68, 0.3)", "value": 95 }
            ]
          }
        },
        "overrides": [
          {
            "matcher": { "id": "byName", "options": "Overall Backpressure" },
            "properties": [
              { "id": "custom.lineWidth", "value": 3 },
              { "id": "color", "value": { "mode": "fixed", "fixedColor": "#8AB8FF" } }
            ]
          }
        ]
      }
    },
    {
      "id": 6,
      "title": "Component Utilization Breakdown",
      "type": "bargauge",
      "gridPos": { "h": 6, "w": 12, "x": 0, "y": 12 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "collab:websocket_utilization:ratio * 100",
          "legendFormat": "WebSocket Connections",
          "refId": "A"
        },
        {
          "expr": "collab:redis_pubsub_utilization:ratio * 100",
          "legendFormat": "Redis Pub/Sub Channels",
          "refId": "B"
        },
        {
          "expr": "collab:ot_queue_utilization:ratio * 100",
          "legendFormat": "OT Operation Queue",
          "refId": "C"
        },
        {
          "expr": "collab:memory_utilization:ratio * 100",
          "legendFormat": "Memory",
          "refId": "D"
        }
      ],
      "options": {
        "reduceOptions": {
          "values": false,
          "calcs": ["lastNotNull"],
          "fields": ""
        },
        "orientation": "horizontal",
        "displayMode": "gradient",
        "showUnfilled": true
      },
      "fieldConfig": {
        "defaults": {
          "unit": "percent",
          "min": 0,
          "max": 100,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              { "color": "#73BF69", "value": null },
              { "color": "#FADE2A", "value": 70 },
              { "color": "#FF9830", "value": 85 },
              { "color": "#E02F44", "value": 95 }
            ]
          }
        }
      }
    },
    {
      "id": 7,
      "title": "Rejected Operations by Reason",
      "type": "timeseries",
      "gridPos": { "h": 6, "w": 12, "x": 12, "y": 12 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "sum by (reason) (rate(collab_operations_rejected_total[5m]))",
          "legendFormat": "{{ reason }}",
          "refId": "A"
        }
      ],
      "options": {
        "tooltip": { "mode": "multi", "sort": "desc" },
        "legend": { "displayMode": "table", "placement": "right", "calcs": ["sum"] }
      },
      "fieldConfig": {
        "defaults": {
          "unit": "ops",
          "custom": {
            "drawStyle": "bars",
            "fillOpacity": 80,
            "stacking": { "mode": "normal", "group": "A" }
          }
        }
      }
    },
    {
      "id": 8,
      "title": "WebSocket Utilization by Pod",
      "type": "timeseries",
      "gridPos": { "h": 6, "w": 12, "x": 0, "y": 18 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "(sum by (pod) (collab_websocket_active_connections) / sum by (pod) (collab_websocket_max_connections)) * 100",
          "legendFormat": "{{ pod }}",
          "refId": "A"
        }
      ],
      "options": {
        "tooltip": { "mode": "multi", "sort": "desc" },
        "legend": { "displayMode": "table", "placement": "right", "calcs": ["lastNotNull", "max"] }
      },
      "fieldConfig": {
        "defaults": {
          "unit": "percent",
          "min": 0,
          "max": 100,
          "custom": {
            "drawStyle": "line",
            "lineInterpolation": "smooth",
            "fillOpacity": 10
          },
          "thresholds": {
            "mode": "absolute",
            "steps": [
              { "color": "transparent", "value": null },
              { "color": "rgba(224, 47, 68, 0.3)", "value": 95 }
            ]
          }
        }
      }
    },
    {
      "id": 9,
      "title": "OT Queue Depth by Workspace (Top 10)",
      "type": "timeseries",
      "gridPos": { "h": 6, "w": 12, "x": 12, "y": 18 },
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "topk(10, sum by (workspace_id) (collab_ot_queue_depth))",
          "legendFormat": "{{ workspace_id }}",
          "refId": "A"
        }
      ],
      "options": {
        "tooltip": { "mode": "multi", "sort": "desc" },
        "legend": { "displayMode": "table", "placement": "right", "calcs": ["lastNotNull", "max"] }
      },
      "fieldConfig": {
        "defaults": {
          "unit": "short",
          "custom": {
            "drawStyle": "line",
            "lineInterpolation": "stepAfter",
            "fillOpacity": 20
          }
        }
      }
    },
    {
      "id": 10,
      "title": "Active Alerts",
      "type": "alertlist",
      "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
      "options": {
        "alertName": "Backpressure|Critical|WebSocket|OT|Redis",
        "dashboardAlerts": false,
        "alertInstanceLabelFilter": "",
        "groupBy": ["alertname", "severity"],
        "groupMode": "default",
        "maxItems": 20,
        "sortOrder": 1,
        "stateFilter": {
          "firing": true,
          "pending": true,
          "noData": false,
          "normal": false,
          "error": true
        }
      }
    },
    {
      "id": 11,
      "title": "Backpressure Event Log",
      "type": "logs",
      "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 },
      "datasource": "Loki",
      "targets": [
        {
          "expr": "{namespace=\"collaboration\"} |= \"backpressure\" | json | level=\"warn\" or level=\"error\"",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "options": {
        "showTime": true,
        "showLabels": true,
        "showCommonLabels": false,
        "wrapLogMessage": true,
        "sortOrder": "Descending",
        "dedupStrategy": "none",
        "enableLogDetails": true
      }
    }
  ],
  "refresh": "10s",
  "schemaVersion": 38,
  "style": "dark",
  "tags": ["collaboration", "backpressure", "critical", "sre"],
  "templating": {
    "list": [
      {
        "current": { "selected": false, "text": "All", "value": "$__all" },
        "datasource": "Prometheus",
        "definition": "label_values(collab_websocket_active_connections, pod)",
        "hide": 0,
        "includeAll": true,
        "label": "Pod",
        "multi": true,
        "name": "pod",
        "options": [],
        "query": "label_values(collab_websocket_active_connections, pod)",
        "refresh": 2,
        "regex": "",
        "skipUrlSync": false,
        "sort": 1,
        "type": "query"
      },
      {
        "current": { "selected": false, "text": "All", "value": "$__all" },
        "datasource": "Prometheus",
        "definition": "label_values(collab_redis_pubsub_channels_active, redis_node)",
        "hide": 0,
        "includeAll": true,
        "label": "Redis Node",
        "multi": true,
        "name": "redis_node",
        "options": [],
        "query": "label_values(collab_redis_pubsub_channels_active, redis_node)",
        "refresh": 2,
        "regex": "",
        "skipUrlSync": false,
        "sort": 1,
        "type": "query"
      }
    ]
  },
  "time": { "from": "now-1h", "to": "now" },
  "timepicker": {
    "refresh_intervals": ["5s", "10s", "30s", "1m", "5m"],
    "time_options": ["5m", "15m", "1h", "6h", "12h", "24h", "2d", "7d"]
  },
  "timezone": "browser",
  "title": "Collaboration Backpressure Monitor",
  "uid": "collab-backpressure",
  "version": 1
}
```

---

## 5. Alertmanager Configuration

### 5.1 Routing and Receivers

```yaml
# File: /etc/alertmanager/alertmanager.yml

global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/XXX/YYY/ZZZ'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

route:
  receiver: 'default'
  group_by: ['alertname', 'severity', 'component']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 4h
  routes:
    # CRITICAL alerts ‚Üí PagerDuty + Slack
    - match:
        severity: critical
      receiver: 'critical-pagerduty'
      group_wait: 0s
      group_interval: 1m
      repeat_interval: 5m
      continue: true
    
    - match:
        severity: critical
      receiver: 'critical-slack'
      group_wait: 0s
      group_interval: 1m
      repeat_interval: 15m

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#platform-alerts'
        send_resolved: true

  - name: 'critical-pagerduty'
    pagerduty_configs:
      - service_key: '<PAGERDUTY_SERVICE_KEY>'
        severity: critical
        description: '{{ .CommonAnnotations.summary }}'
        details:
          firing: '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
          num_firing: '{{ .Alerts.Firing | len }}'
          dashboard: '{{ (index .Alerts 0).Annotations.dashboard_url }}'
          runbook: '{{ (index .Alerts 0).Annotations.runbook_url }}'

  - name: 'critical-slack'
    slack_configs:
      - channel: '#platform-critical'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        title: 'üî• {{ .CommonAnnotations.summary }}'
        text: |
          {{ .CommonAnnotations.description }}
          
          *Dashboard:* {{ (index .Alerts 0).Annotations.dashboard_url }}
          *Runbook:* {{ (index .Alerts 0).Annotations.runbook_url }}
        actions:
          - type: button
            text: 'View Dashboard'
            url: '{{ (index .Alerts 0).Annotations.dashboard_url }}'
          - type: button
            text: 'View Runbook'
            url: '{{ (index .Alerts 0).Annotations.runbook_url }}'
```

### 5.2 Notification Templates

```yaml
# File: /etc/alertmanager/templates/collaboration.tmpl

{{ define "collaboration.critical.title" }}
üî• CRITICAL: {{ .CommonAnnotations.summary }}
{{ end }}

{{ define "collaboration.critical.text" }}
*Alert:* {{ .CommonAnnotations.summary }}
*Severity:* {{ .CommonLabels.severity }}
*Component:* {{ .CommonLabels.component }}

{{ .CommonAnnotations.description }}

*Active Alerts:*
{{ range .Alerts.Firing }}
  - {{ .Labels.alertname }}: {{ .Annotations.summary }}
{{ end }}

*Actions:*
1. Check dashboard: {{ (index .Alerts 0).Annotations.dashboard_url }}
2. Follow runbook: {{ (index .Alerts 0).Annotations.runbook_url }}
3. Escalate if not resolved in 5 minutes
{{ end }}

{{ define "collaboration.critical.pagerduty" }}
{
  "routing_key": "{{ .CommonLabels.escalation_policy }}",
  "event_action": "{{ if eq .Status "firing" }}trigger{{ else }}resolve{{ end }}",
  "dedup_key": "{{ .CommonLabels.alertname }}-{{ .CommonLabels.component }}",
  "payload": {
    "summary": "{{ .CommonAnnotations.summary }}",
    "severity": "critical",
    "source": "prometheus",
    "component": "{{ .CommonLabels.component }}",
    "group": "collaboration",
    "custom_details": {
      "description": "{{ .CommonAnnotations.description }}",
      "dashboard": "{{ (index .Alerts 0).Annotations.dashboard_url }}",
      "runbook": "{{ (index .Alerts 0).Annotations.runbook_url }}"
    }
  }
}
{{ end }}
```

---

## 6. Emergency Response Automation

### 6.1 Auto-Scaling Trigger

```yaml
# File: /etc/prometheus/rules/collaboration_autoscale.yml

groups:
  - name: collaboration_autoscale_triggers
    interval: 15s
    rules:
      # Trigger HPA scale-up when approaching CRITICAL
      - alert: CollaborationScaleUpNeeded
        expr: |
          collab:backpressure_level:ratio > 0.85
          and
          deriv(collab:backpressure_level:ratio[5m]) > 0
        for: 2m
        labels:
          severity: info
          action: scale_up
        annotations:
          summary: "Scale-up recommended: backpressure at {{ $value | humanizePercentage }}"
          webhook_payload: |
            {
              "action": "scale_up",
              "target": "collaboration-websocket",
              "current_replicas": "{{ with query \"kube_deployment_spec_replicas{deployment='collaboration-websocket'}\" }}{{ . | first | value }}{{ end }}",
              "reason": "backpressure_high"
            }
```

### 6.2 Circuit Breaker Webhook

```python
# File: /opt/collaboration/webhooks/backpressure_handler.py

from flask import Flask, request, jsonify
import redis
import logging

app = Flask(__name__)
redis_client = redis.Redis(host='redis-master', port=6379, db=0)
logger = logging.getLogger(__name__)

@app.route('/webhook/backpressure', methods=['POST'])
def handle_backpressure_alert():
    """
    Handle backpressure alerts from Alertmanager.
    Activates circuit breaker when CRITICAL threshold reached.
    """
    data = request.json
    
    for alert in data.get('alerts', []):
        if alert['status'] == 'firing':
            severity = alert['labels'].get('severity')
            
            if severity == 'critical':
                # Activate circuit breaker
                activate_circuit_breaker()
                
                # Notify operations
                notify_operations(alert)
                
                # Log incident
                log_incident(alert)
        
        elif alert['status'] == 'resolved':
            # Deactivate circuit breaker (with delay)
            schedule_circuit_breaker_deactivation(delay_seconds=120)
    
    return jsonify({'status': 'processed'}), 200


def activate_circuit_breaker():
    """
    Activate circuit breaker to reject new connections.
    """
    logger.critical("ACTIVATING CIRCUIT BREAKER - Rejecting new connections")
    
    # Set Redis flag
    redis_client.set('collab:circuit_breaker:active', '1')
    redis_client.set('collab:circuit_breaker:activated_at', str(time.time()))
    
    # Publish to all WebSocket servers
    redis_client.publish('collab:circuit_breaker', json.dumps({
        'action': 'activate',
        'reason': 'backpressure_critical',
        'timestamp': time.time()
    }))


def schedule_circuit_breaker_deactivation(delay_seconds: int):
    """
    Schedule circuit breaker deactivation after delay.
    """
    logger.info(f"Scheduling circuit breaker deactivation in {delay_seconds}s")
    
    # Use Redis sorted set for delayed execution
    redis_client.zadd(
        'collab:circuit_breaker:deactivate_queue',
        {str(uuid.uuid4()): time.time() + delay_seconds}
    )


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

---

## 7. Runbook: CRITICAL Backpressure Response

### 7.1 Immediate Actions (0-5 minutes)

```markdown
## CRITICAL Backpressure Response Runbook

### Step 1: Acknowledge Alert (0-1 min)
- [ ] Acknowledge PagerDuty alert
- [ ] Join #incident-collab Slack channel
- [ ] Open dashboard: https://grafana.internal/d/collab-backpressure

### Step 2: Identify Bottleneck (1-3 min)
Check which component is at CRITICAL:

```bash
# WebSocket connections
curl -s http://prometheus:9090/api/v1/query?query=collab:websocket_utilization:ratio | jq '.data.result[0].value[1]'

# Redis pub/sub
curl -s http://prometheus:9090/api/v1/query?query=collab:redis_pubsub_utilization:ratio | jq '.data.result[0].value[1]'

# OT queue
curl -s http://prometheus:9090/api/v1/query?query=collab:ot_queue_utilization:ratio | jq '.data.result[0].value[1]'

# Memory
curl -s http://prometheus:9090/api/v1/query?query=collab:memory_utilization:ratio | jq '.data.result[0].value[1]'
```

### Step 3: Apply Immediate Mitigation (3-5 min)

**If WebSocket exhaustion:**
```bash
# Scale up WebSocket pods
kubectl scale deployment collaboration-websocket --replicas=10 -n collaboration
```

**If Redis saturation:**
```bash
# Add Redis node (if cluster)
redis-cli --cluster add-node <new-node>:6379 <existing-node>:6379

# Or restart to clear channels
kubectl rollout restart statefulset redis -n collaboration
```

**If OT queue overflow:**
```bash
# Identify flooding workspace
curl -s 'http://prometheus:9090/api/v1/query?query=topk(5,sum by (workspace_id)(collab_ot_queue_depth))' | jq

# Throttle specific workspace
curl -X POST http://collab-api:8080/admin/workspace/<id>/throttle -d '{"ops_per_second": 10}'
```

**If memory pressure:**
```bash
# Restart pods one by one
kubectl rollout restart deployment collaboration-websocket -n collaboration
```

### Step 4: Verify Recovery (5-10 min)
- [ ] Backpressure level dropping
- [ ] No new rejected operations
- [ ] Circuit breaker deactivated
- [ ] Users can connect

### Step 5: Post-Incident
- [ ] Create incident report
- [ ] Schedule post-mortem
- [ ] Update capacity planning
```

---

## 8. Testing the Alert

### 8.1 Synthetic Load Test

```python
# File: /opt/collaboration/tests/backpressure_load_test.py

import asyncio
import websockets
import json
import time
from prometheus_client import start_http_server, Counter, Gauge

# Metrics
connections_created = Counter('loadtest_connections_created_total', 'Connections created')
connections_rejected = Counter('loadtest_connections_rejected_total', 'Connections rejected')
current_connections = Gauge('loadtest_current_connections', 'Current connections')

async def create_connection(workspace_id: str, user_id: str):
    """Create a WebSocket connection and send operations."""
    uri = f"wss://collab.internal/ws/{workspace_id}"
    
    try:
        async with websockets.connect(uri, extra_headers={'X-User-ID': user_id}) as ws:
            connections_created.inc()
            current_connections.inc()
            
            # Send operations at high rate
            while True:
                op = {
                    'type': 'INSERT',
                    'position': 0,
                    'text': 'x' * 100,
                    'version': int(time.time() * 1000)
                }
                await ws.send(json.dumps(op))
                await asyncio.sleep(0.01)  # 100 ops/sec per connection
                
    except websockets.exceptions.ConnectionClosed as e:
        if e.code == 1013:  # Try again later (backpressure)
            connections_rejected.inc()
        current_connections.dec()
    except Exception as e:
        current_connections.dec()


async def load_test(target_connections: int, workspace_id: str):
    """Run load test to trigger backpressure."""
    print(f"Starting load test: {target_connections} connections to workspace {workspace_id}")
    
    tasks = []
    for i in range(target_connections):
        user_id = f"loadtest-user-{i}"
        task = asyncio.create_task(create_connection(workspace_id, user_id))
        tasks.append(task)
        await asyncio.sleep(0.01)  # Ramp up gradually
    
    await asyncio.gather(*tasks, return_exceptions=True)


if __name__ == '__main__':
    # Start metrics server
    start_http_server(8000)
    
    # Run load test
    asyncio.run(load_test(
        target_connections=15000,  # Exceed 10k limit
        workspace_id='loadtest-workspace'
    ))
```

### 8.2 Alert Verification

```bash
# Verify alert is firing
curl -s http://prometheus:9090/api/v1/alerts | jq '.data.alerts[] | select(.labels.alertname | contains("Backpressure"))'

# Verify Alertmanager received it
curl -s http://alertmanager:9093/api/v2/alerts | jq '.[] | select(.labels.alertname | contains("Backpressure"))'

# Verify PagerDuty incident created
# Check PagerDuty dashboard or API

# Verify Slack notification sent
# Check #platform-critical channel
```

---

## 9. Metric Definitions Reference

| Metric Name | Type | Labels | Description |
|-------------|------|--------|-------------|
| `collab_websocket_active_connections` | Gauge | pod | Current WebSocket connections |
| `collab_websocket_max_connections` | Gauge | pod | Max WebSocket connections |
| `collab_redis_pubsub_channels_active` | Gauge | redis_node | Active pub/sub channels |
| `collab_redis_pubsub_channels_max` | Gauge | redis_node | Max pub/sub channels |
| `collab_ot_queue_depth` | Gauge | workspace_id | Current OT queue depth |
| `collab_ot_queue_capacity` | Gauge | workspace_id | Max OT queue capacity |
| `collab_operations_rejected_total` | Counter | reason | Operations rejected |
| `collab_websocket_connections_rejected_total` | Counter | reason | Connections rejected |
| `collab:backpressure_level:ratio` | Recording | - | Overall backpressure (0-1) |
| `collab:backpressure_category:level` | Recording | - | Category (0-3) |

---

## 10. Document Stats

| Section | Lines | Code Examples |
|---------|-------|---------------|
| Alert Rules | 200+ | 5 rules |
| Recording Rules | 80+ | 8 rules |
| PromQL Queries | 100+ | 12 queries |
| Grafana Dashboard | 400+ | 11 panels |
| Alertmanager Config | 100+ | 3 receivers |
| Automation | 100+ | 2 scripts |
| Runbook | 80+ | 5 steps |
| Testing | 80+ | 2 tests |
| **Total** | **1140+** | **Complete** |

---

**Production-ready CRITICAL backpressure alerting implementation.** üéØ
