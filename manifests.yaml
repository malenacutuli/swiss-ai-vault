# =============================================================================
# BullMQ Worker Service - Complete Kubernetes Manifests
# =============================================================================
# Author: Manus AI
# Date: January 21, 2026
# Version: 1.0
# Status: Production-Ready
#
# Prerequisites:
#   - Redis Cluster deployed (REDIS_CLUSTER_KUBERNETES_MANIFESTS.yaml)
#   - Container registry with bullmq-worker image
#   - Prometheus operator (for ServiceMonitor)
#
# Deployment Order:
#   1. kubectl apply -f BULLMQ_WORKER_KUBERNETES_MANIFESTS.yaml
#   2. kubectl rollout status deployment/bullmq-worker -n workers
#   3. kubectl logs -f deployment/bullmq-worker -n workers
# =============================================================================

---
# =============================================================================
# 00-namespace.yaml - Namespace
# =============================================================================
apiVersion: v1
kind: Namespace
metadata:
  name: workers
  labels:
    name: workers
    app.kubernetes.io/name: bullmq-workers
    app.kubernetes.io/component: worker

---
# =============================================================================
# 01-serviceaccount.yaml - Service Account
# =============================================================================
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bullmq-worker
  namespace: workers
  labels:
    app: bullmq-worker

---
# =============================================================================
# 02-secrets.yaml - Worker Secrets
# =============================================================================
apiVersion: v1
kind: Secret
metadata:
  name: bullmq-worker-secrets
  namespace: workers
type: Opaque
data:
  # Redis worker password (must match redis-secrets in redis namespace)
  # echo -n 'worker-super-secret-password-345678' | base64
  redis-password: d29ya2VyLXN1cGVyLXNlY3JldC1wYXNzd29yZC0zNDU2Nzg=
  
  # Database connection string (replace with your actual connection string)
  # echo -n 'postgresql://user:pass@host:5432/db?sslmode=require' | base64
  database-url: cG9zdGdyZXNxbDovL3VzZXI6cGFzc0Bob3N0OjU0MzIvZGI/c3NsbW9kZT1yZXF1aXJl
  
  # LLM API Key (OpenAI/Anthropic)
  # echo -n 'sk-...' | base64
  llm-api-key: c2stcGxhY2Vob2xkZXItYXBpLWtleQ==
  
  # S3 credentials
  s3-access-key: QUtJQVBMQUNFSE9MREVSCg==
  s3-secret-key: c2VjcmV0LXBsYWNlaG9sZGVyLWtleQ==

---
# =============================================================================
# 03-configmap.yaml - Worker Configuration
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: bullmq-worker-config
  namespace: workers
data:
  # Worker configuration
  WORKER_CONCURRENCY: "10"
  WORKER_MAX_JOBS_PER_WORKER: "100"
  WORKER_STALLED_INTERVAL: "30000"
  WORKER_LOCK_DURATION: "60000"
  WORKER_LOCK_RENEW_TIME: "15000"
  
  # Queue configuration
  QUEUE_PREFIX: "bull"
  QUEUE_DEFAULT_JOB_OPTIONS: |
    {
      "attempts": 3,
      "backoff": {
        "type": "exponential",
        "delay": 1000
      },
      "removeOnComplete": {
        "age": 604800,
        "count": 10000
      },
      "removeOnFail": {
        "age": 2592000,
        "count": 50000
      }
    }
  
  # Redis configuration
  REDIS_HOST: "redis-cluster.redis.svc.cluster.local"
  REDIS_PORT: "6379"
  REDIS_USERNAME: "worker"
  REDIS_TLS_ENABLED: "true"
  REDIS_CLUSTER_ENABLED: "true"
  REDIS_MAX_RETRIES_PER_REQUEST: "3"
  REDIS_CONNECT_TIMEOUT: "10000"
  REDIS_COMMAND_TIMEOUT: "5000"
  
  # Sandbox configuration
  SANDBOX_POOL_SIZE: "5"
  SANDBOX_IDLE_TIMEOUT: "300000"
  SANDBOX_MAX_EXECUTION_TIME: "3600000"
  SANDBOX_PROVIDER: "e2b"
  
  # LLM configuration
  LLM_PROVIDER: "openai"
  LLM_MODEL: "gpt-4-turbo"
  LLM_MAX_TOKENS: "4096"
  LLM_TEMPERATURE: "0.7"
  
  # S3 configuration
  S3_BUCKET: "swissbrain-artifacts"
  S3_REGION: "eu-central-1"
  S3_ENDPOINT: ""
  
  # Logging
  LOG_LEVEL: "info"
  LOG_FORMAT: "json"
  
  # Metrics
  METRICS_ENABLED: "true"
  METRICS_PORT: "9090"
  METRICS_PATH: "/metrics"

---
# =============================================================================
# 04-configmap-tls.yaml - TLS Certificates (copied from Redis)
# =============================================================================
# In production, use a shared secret or cert-manager
apiVersion: v1
kind: Secret
metadata:
  name: redis-tls-client
  namespace: workers
type: kubernetes.io/tls
data:
  # Copy these from redis-tls secret in redis namespace
  # kubectl get secret redis-tls -n redis -o yaml | grep -E '^\s+tls\.' 
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURwVENDQW8yZ0F3SUJBZ0lVYkxGbHZGbHZGbHZGbHZGbHZGbHZGbHZGbHYwd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lqRUxNQWtHQTFVRUJoTUNWVk14Q3pBSkJnTlZCQWdNQWtOQk1SSXdFQVlEVlFRSERBbFRZVzRnU205egpaVEVOTUFzR0ExVUVDZ3dFVkdWemRERU5NQXNHQTFVRUN3d0VWR1Z6ZERFVU1CSUdBMVVFQXd3TGNtVmthWE10ClkyeDFjM1JsY2pBZUZ3MHlOREF4TWpFeE1qQXdNREJhRncweU5UQXhNakF4TWpBd01EQmFNR0l4Q3pBSkJnTlYKQkFZVEFsVlRNUXN3Q1FZRFZRUUlEQUpEUVRFU01CQUdBMVVFQnd3SlUyRnVJRXB2YzJVeERUQUxCZ05WQkFvTQpCRlJsYzNReERUQUxCZ05WQkFzTUJGUmxjM1F4RkRBU0JnTlZCQU1NQzNKbFpHbHpMV05zZFhOMFpYSXdnZ0VpCk1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLQW9JQkFRREZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEYKbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsCkZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmwKRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbApGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsCkZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmwKRmxGbEZsRmxGbEZsQWdNQkFBR2pVekJSTUIwR0ExVWREZ1FXQkJRQUFBQUFBQUFBQUFBQUFBQUFBQUFBQURBZgpCZ05WSFNNRUdEQVdnQlFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBREFQQmdOVkhSTUJBZjhFQlRBREFRSC9NQTBHCkNTcUdTSWIzRFFFQkN3VUFBNElCQVFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQQpBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBCkFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
  tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2Z0lCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQktnd2dnU2tBZ0VBQW9JQkFRREZsRmxGbEZsRmxGbEYKbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsCkZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmwKRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbApGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsCkZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmxGbEZsRmwKRmxGbEZsRmxGbEZsQWdNQkFBRUNnZ0VBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQQpBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBCkFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQQpBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBCkFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQQpBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBCkFBQUFBQUFBQUFBQUFBQUFBUUtCZ1FBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKLS0tLS1FTkQgUFJJVkFURSBLRVktLS0tLQo=
  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURwVENDQW8yZ0F3SUJBZ0lVYkxGbHZGbHZGbHZGbHZGbHZGbHZGbHZGbHYwd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lqRUxNQWtHQTFVRUJoTUNWVk14Q3pBSkJnTlZCQWdNQWtOQk1SSXdFQVlEVlFRSERBbFRZVzRnU205egpaVEVOTUFzR0ExVUVDZ3dFVkdWemRERU5NQXNHQTFVRUN3d0VWR1Z6ZERFVU1CSUdBMVVFQXd3TGNtVmthWE10ClkyRXdIaGNOTWpRd01USXhNVEl3TURBd1doY05Namt3TVRJd01USXdNREF3V2pCaU1Rc3dDUVlEVlFRR0V3SlYKVXpFTE1Ba0dBMVVFQ0F3Q1EwRXhFakFRQmdOVkJBY01DVk5oYmlCS2IzTmxNUTB3Q3dZRFZRUUtEQVJVWlhOMApNUTB3Q3dZRFZRUUxEQVJVWlhOME1SUXdFZ1lEVlFRRERBdHlaV1JwY3kxallUQ0NBU0l3RFFZSktvWklodmNOCkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNV1VXVVdVV1VXVVdVV1VXVVdVV1VXVVdVV1VXVVdVV1VXVVdVV1UKVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVUKVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVUKVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVUKVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVUKVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVVXVVUKVVVXVVVXVVVXVVVXQWDNQUFHZ1v6QlJNQjBHQTFVZERnUVdCQlFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBREFmCkJnTlZIU01FR0RBV2dCUUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFEQVBCZ05WSFJNQkFmOEVCVEFEQVFIL01BMEcKQ1NxR1NJYjNEUUVCQ3dVQUE0SUFBUUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUEKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=

---
# =============================================================================
# 05-deployment.yaml - BullMQ Worker Deployment
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bullmq-worker
  namespace: workers
  labels:
    app: bullmq-worker
    app.kubernetes.io/name: bullmq-worker
    app.kubernetes.io/component: worker
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: bullmq-worker
  template:
    metadata:
      labels:
        app: bullmq-worker
        app.kubernetes.io/name: bullmq-worker
        app.kubernetes.io/component: worker
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      terminationGracePeriodSeconds: 300
      serviceAccountName: bullmq-worker
      
      # Image Pull Secrets for GHCR
      imagePullSecrets:
        - name: ghcr-secret
      
      # Security Context
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        runAsNonRoot: true
      
      # Pod Anti-Affinity for HA
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: bullmq-worker
                topologyKey: kubernetes.io/hostname
      
      # Topology Spread
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: bullmq-worker
      
      containers:
        # -----------------------------
        # BullMQ Worker Container
        # -----------------------------
        - name: worker
          image: ghcr.io/malenacutuli/swiss-ai-vault:latest
          imagePullPolicy: Always
          
          # Environment Variables
          env:
            # Redis connection
            - name: REDIS_HOST
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: REDIS_HOST
            - name: REDIS_PORT
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: REDIS_PORT
            - name: REDIS_USERNAME
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: REDIS_USERNAME
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: bullmq-worker-secrets
                  key: redis-password
            - name: REDIS_TLS_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: REDIS_TLS_ENABLED
            - name: REDIS_CLUSTER_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: REDIS_CLUSTER_ENABLED
            - name: REDIS_TLS_CA_PATH
              value: /etc/redis-tls/ca.crt
            - name: REDIS_TLS_CERT_PATH
              value: /etc/redis-tls/tls.crt
            - name: REDIS_TLS_KEY_PATH
              value: /etc/redis-tls/tls.key
            
            # Worker configuration
            - name: WORKER_CONCURRENCY
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: WORKER_CONCURRENCY
            - name: WORKER_MAX_JOBS_PER_WORKER
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: WORKER_MAX_JOBS_PER_WORKER
            - name: WORKER_STALLED_INTERVAL
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: WORKER_STALLED_INTERVAL
            - name: WORKER_LOCK_DURATION
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: WORKER_LOCK_DURATION
            - name: WORKER_LOCK_RENEW_TIME
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: WORKER_LOCK_RENEW_TIME
            
            # Queue configuration
            - name: QUEUE_PREFIX
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: QUEUE_PREFIX
            
            # Database
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: bullmq-worker-secrets
                  key: database-url
            
            # LLM
            - name: LLM_API_KEY
              valueFrom:
                secretKeyRef:
                  name: bullmq-worker-secrets
                  key: llm-api-key
            - name: LLM_PROVIDER
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: LLM_PROVIDER
            - name: LLM_MODEL
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: LLM_MODEL
            
            # S3
            - name: S3_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: bullmq-worker-secrets
                  key: s3-access-key
            - name: S3_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: bullmq-worker-secrets
                  key: s3-secret-key
            - name: S3_BUCKET
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: S3_BUCKET
            - name: S3_REGION
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: S3_REGION
            
            # Sandbox
            - name: SANDBOX_POOL_SIZE
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: SANDBOX_POOL_SIZE
            - name: SANDBOX_PROVIDER
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: SANDBOX_PROVIDER
            
            # Logging
            - name: LOG_LEVEL
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: LOG_LEVEL
            - name: LOG_FORMAT
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: LOG_FORMAT
            
            # Metrics
            - name: METRICS_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: METRICS_ENABLED
            - name: METRICS_PORT
              valueFrom:
                configMapKeyRef:
                  name: bullmq-worker-config
                  key: METRICS_PORT
            
            # Pod info
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          
          ports:
            - containerPort: 9090
              name: metrics
              protocol: TCP
            - containerPort: 8080
              name: health
              protocol: TCP
          
          resources:
            requests:
              cpu: "2"
              memory: 4Gi
            limits:
              cpu: "4"
              memory: 8Gi
          
          volumeMounts:
            - name: redis-tls
              mountPath: /etc/redis-tls
              readOnly: true
            - name: tmp
              mountPath: /tmp
            - name: cache
              mountPath: /home/worker/.cache
          
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
          
          # Liveness Probe
          livenessProbe:
            httpGet:
              path: /health/live
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          
          # Readiness Probe
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          
          # Startup Probe
          startupProbe:
            httpGet:
              path: /health/startup
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 5
            failureThreshold: 30
            successThreshold: 1
          
          # Lifecycle hooks for graceful shutdown
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    echo "Received SIGTERM, initiating graceful shutdown..."
                    # Signal worker to stop accepting new jobs
                    touch /tmp/shutdown-requested
                    # Wait for current jobs to complete (max 5 minutes)
                    timeout 300 sh -c 'while [ -f /tmp/jobs-in-progress ]; do sleep 1; done'
                    echo "Graceful shutdown complete"
      
      volumes:
        - name: redis-tls
          secret:
            secretName: redis-tls-client
        - name: tmp
          emptyDir: {}
        - name: cache
          emptyDir:
            sizeLimit: 1Gi

---
# =============================================================================
# 06-service.yaml - Worker Service (for metrics)
# =============================================================================
apiVersion: v1
kind: Service
metadata:
  name: bullmq-worker
  namespace: workers
  labels:
    app: bullmq-worker
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
spec:
  type: ClusterIP
  ports:
    - port: 9090
      targetPort: 9090
      name: metrics
      protocol: TCP
    - port: 8080
      targetPort: 8080
      name: health
      protocol: TCP
  selector:
    app: bullmq-worker

---
# =============================================================================
# 07-hpa.yaml - Horizontal Pod Autoscaler
# =============================================================================
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: bullmq-worker-hpa
  namespace: workers
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: bullmq-worker
  minReplicas: 3
  maxReplicas: 20
  metrics:
    # Scale based on CPU
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    # Scale based on memory
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    # Scale based on queue depth (requires Prometheus adapter)
    - type: External
      external:
        metric:
          name: bullmq_queue_waiting_total
          selector:
            matchLabels:
              queue: tasks
        target:
          type: AverageValue
          averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
        - type: Percent
          value: 50
          periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120
      selectPolicy: Min

---
# =============================================================================
# 08-pdb.yaml - Pod Disruption Budget
# =============================================================================
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: bullmq-worker-pdb
  namespace: workers
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: bullmq-worker

---
# =============================================================================
# 09-servicemonitor.yaml - Prometheus ServiceMonitor
# =============================================================================
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: bullmq-worker
  namespace: workers
  labels:
    app: bullmq-worker
    release: prometheus
spec:
  selector:
    matchLabels:
      app: bullmq-worker
  namespaceSelector:
    matchNames:
      - workers
  endpoints:
    - port: metrics
      interval: 15s
      scrapeTimeout: 10s
      path: /metrics

---
# =============================================================================
# 10-networkpolicy.yaml - Network Policy
# =============================================================================
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: bullmq-worker-network-policy
  namespace: workers
spec:
  podSelector:
    matchLabels:
      app: bullmq-worker
  policyTypes:
    - Ingress
    - Egress
  
  ingress:
    # Allow Prometheus scraping
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
          podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9090
    
    # Allow health checks from anywhere in cluster
    - from:
        - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 8080
  
  egress:
    # Allow Redis cluster access
    - to:
        - namespaceSelector:
            matchLabels:
              name: redis
          podSelector:
            matchLabels:
              app: redis
      ports:
        - protocol: TCP
          port: 6379
    
    # Allow database access
    - to:
        - namespaceSelector:
            matchLabels:
              name: database
      ports:
        - protocol: TCP
          port: 5432
    
    # Allow external API access (LLM, S3, etc.)
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              - 10.0.0.0/8
              - 172.16.0.0/12
              - 192.168.0.0/16
      ports:
        - protocol: TCP
          port: 443
    
    # Allow DNS
    - to:
        - namespaceSelector: {}
          podSelector:
            matchLabels:
              k8s-app: kube-dns
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53

---
# =============================================================================
# 11-prometheusrule.yaml - Prometheus Alert Rules
# =============================================================================
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: bullmq-worker-alerts
  namespace: workers
  labels:
    app: bullmq-worker
    release: prometheus
spec:
  groups:
    - name: bullmq-worker.rules
      rules:
        # High queue depth
        - alert: BullMQHighQueueDepth
          expr: |
            sum(bullmq_queue_waiting_total{namespace="workers"}) > 1000
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High BullMQ queue depth"
            description: "Queue depth is {{ $value }} jobs, exceeding threshold of 1000"
        
        # Critical queue depth
        - alert: BullMQCriticalQueueDepth
          expr: |
            sum(bullmq_queue_waiting_total{namespace="workers"}) > 5000
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Critical BullMQ queue depth"
            description: "Queue depth is {{ $value }} jobs, exceeding critical threshold of 5000"
        
        # High job failure rate
        - alert: BullMQHighFailureRate
          expr: |
            sum(rate(bullmq_jobs_failed_total{namespace="workers"}[5m])) 
            / sum(rate(bullmq_jobs_completed_total{namespace="workers"}[5m])) > 0.1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High BullMQ job failure rate"
            description: "Job failure rate is {{ $value | humanizePercentage }}"
        
        # Worker pod down
        - alert: BullMQWorkerDown
          expr: |
            up{job="bullmq-worker", namespace="workers"} == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "BullMQ worker pod is down"
            description: "Worker pod {{ $labels.pod }} is not responding"
        
        # Stalled jobs
        - alert: BullMQStalledJobs
          expr: |
            sum(bullmq_queue_stalled_total{namespace="workers"}) > 10
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Stalled BullMQ jobs detected"
            description: "{{ $value }} jobs are stalled"
        
        # High job latency
        - alert: BullMQHighJobLatency
          expr: |
            histogram_quantile(0.95, 
              sum by (le) (rate(bullmq_job_duration_seconds_bucket{namespace="workers"}[5m]))
            ) > 300
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High BullMQ job latency"
            description: "P95 job duration is {{ $value | humanizeDuration }}"
        
        # Redis connection errors
        - alert: BullMQRedisConnectionErrors
          expr: |
            sum(rate(bullmq_redis_connection_errors_total{namespace="workers"}[5m])) > 1
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "BullMQ Redis connection errors"
            description: "Redis connection error rate is {{ $value }}/s"

---
# =============================================================================
# 12-role.yaml - RBAC Role
# =============================================================================
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: bullmq-worker-role
  namespace: workers
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]

---
# =============================================================================
# 13-rolebinding.yaml - RBAC RoleBinding
# =============================================================================
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: bullmq-worker-rolebinding
  namespace: workers
subjects:
  - kind: ServiceAccount
    name: bullmq-worker
    namespace: workers
roleRef:
  kind: Role
  name: bullmq-worker-role
  apiGroup: rbac.authorization.k8s.io
